{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c67747c9-ed3f-4cc7-896c-a29cbe30656f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "551af5cb-46c1-457f-9e11-7252f9038e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_yaml_file(file, data):\n",
    "    with open(file, \"w\") as f:\n",
    "        f.writelines(x + '\\n' for x in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f32ddca-7cd7-400b-8f1a-0043584acb99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pkl_dict(filename, folder):\n",
    "    print(f\"Loaded_file {filename}\")\n",
    "    with open(folder + filename, \"rb\") as f:\n",
    "        loaded_file = pickle.load(f)\n",
    "        return loaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dea32523-6b0b-49d4-bf7c-4859343ed1da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_pkl(filename,data, folder):\n",
    "    with open(folder + filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86fae3ba-f26a-4fae-b77a-3e055977b102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dict_to_file(options, path):\n",
    "    data = []\n",
    "    for opt in options.keys():\n",
    "        data.append(f\"{opt}: {str(options[opt])}\")\n",
    "    \n",
    "    if options['type'] == \"base\":\n",
    "        file_name = f\"params_{options['type']}_{options['alphas'][0]}_{options['negative_fix'][0]}_{options['normalize_factor']}_{options['tail_sizes'][0]}-{options['tail_sizes'][-1]}_{options['distance_multipls'][0]}-{options['distance_multipls'][-1]}.yaml\"\n",
    "    else:\n",
    "        file_name = f\"params_{options['type']}_{options['alphas'][0]}_{options['negative_fix']}_{options['normalize_factor']}_{options['tail_sizes'][0]}_{options['distance_multipls'][0]}_{options['num_clusters_per_class_input'][0]}-{options['num_clusters_per_class_input'][-1]}_{options['num_clusters_per_class_features'][0]}-{options['num_clusters_per_class_features'][-1]}.yaml\"\n",
    "\n",
    "    write_yaml_file(path + file_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17ba612c-b042-4e98-a1a2-18ae048040c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"type\": \"\" ,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_clusters_per_class_input\": [],\n",
    "    \"num_clusters_per_class_features\": [],\n",
    "    \"alphas\": \"\",\n",
    "    \"tail_sizes\" : [],\n",
    "    \"distance_multipls\": [],\n",
    "    \"negative_fix\": \"\",\n",
    "    \"normalize_factor\": \"\",\n",
    "    \"logger_output\": \"true\",\n",
    "    \"train_only\": \"false\",\n",
    "    \"eval_only\": \"true\",\n",
    "    \"log_dir\": \"./logs/\",\n",
    "    \"saved_models_dir\": \"./saved_models/pytorch_models/\",\n",
    "    \"saved_network_output_dir\": \"./saved_models/network_outputs/\",\n",
    "    \"experiment_data_dir\": \"./experiment_data/\",\n",
    "    \"emnist_dir\": \"./downloads/\",\n",
    "    \"thresholds\": [0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 0.7, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae759fd4-a269-4e87-9619-ce205875264e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "types = [\"base\", \"input-cluster\", \"validation-features-cluster\", \"training-features-cluster\", \"input-validation-features-cluster\", \"input-validation-features-cluster\"]\n",
    "negative_fixes = [\"ORIGINAL\", \"NEGATIVE_VALUE\", \"VALUE_SHIFT\"]\n",
    "norm = [\"NONE\", \"WEIGHTS\", \"N-CLASSES\", \"NORM-WEIGHTS\"]\n",
    "path = \"../configs/base/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5371604-3b15-4785-8e9b-a7ea4be48f53",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bb637cc-e141-487f-8bf3-03fa3ebfdde6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900], [4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900], [5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900], [6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900], [7000]]\n",
      "[[1.05, 1.0, 1.1, 1.2, 1.3], [1.4, 1.5, 1.6, 1.7, 1.8], [1.9, 2.0, 2.1, 2.2, 2.3], [2.4, 2.5, 2.6, 2.7, 2.8], [2.9, 3.0, 3.1, 3.2, 3.3], [3.4, 3.5, 3.6, 3.7, 3.8], [3.9, 4.0, 4.1, 4.2, 4.3], [4.4, 4.5, 4.6, 4.7, 4.8], [4.9, 5.0]]\n"
     ]
    }
   ],
   "source": [
    "typ = types[0]\n",
    "alphas = [-1, 3, 5, 7, 10]\n",
    "tail_sizes = np.arange(3000, 7100, 100).tolist()\n",
    "n = 10\n",
    "per_file_tail_sizes = [tail_sizes[i:i + n] for i in range(0, len(tail_sizes), n)]\n",
    "print(per_file_tail_sizes)\n",
    "dist_multpl = [1.05] + np.arange(1, 5.1, 0.1).tolist()\n",
    "n = 5\n",
    "per_file_dist = [np.round(dist_multpl[i:i + n],2).tolist() for i in range(0, len(dist_multpl), n)]\n",
    "print(per_file_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d4fbd98-24e0-42b7-9dc1-de0b5c6d1368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    for a in alphas:\n",
    "        for nf in negative_fixes:\n",
    "            for no in norm:\n",
    "                for ts in per_file_tail_sizes:\n",
    "                    for d in per_file_dist:\n",
    "                        for n in norm:\n",
    "                            options['type'] = typ\n",
    "                            options['alphas'] = [a]\n",
    "                            options['tail_sizes'] = ts\n",
    "                            options['distance_multipls'] = d\n",
    "                            options['negative_fix'] = [nf]\n",
    "                            options['normalize_factor'] = n\n",
    "                            dict_to_file(options, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab22d2c1-e288-4b90-a547-a210157cf757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def condense_exp(alphas, negative_fixes, norms):\n",
    "    ls = os.listdir('../../experiment_data/batch04/')\n",
    "    keys_of_interest = [\"ACC\", \"CCR-FPR\", \"GAMMA\", \"EPSILON\"]\n",
    "    folder = \"../../experiment_data/batch04/\"\n",
    "    folder_write = \"../../experiment_data/results/\"\n",
    "    for alpha in alphas:\n",
    "        for negative_fix in negative_fixes:\n",
    "            for norm in norms:\n",
    "                prefix = f\"oscr_data_base_1_1_{alpha}_{negative_fix}_{norm}\"\n",
    "                files = [filename for filename in ls if filename.startswith(prefix)]\n",
    "                \n",
    "                condensed_dict = {}\n",
    "                for k in keys_of_interest:\n",
    "                    condensed_dict[k] = {}\n",
    "                    \n",
    "                for file in files:\n",
    "                    if file != prefix + \".pkl\":\n",
    "                        loaded_file = load_pkl_dict(file, folder)\n",
    "                        for key in keys_of_interest:\n",
    "                            condensed_dict[key] = {**condensed_dict[key], **loaded_file[key]}\n",
    "                write_pkl(prefix + \".pkl\", condensed_dict, folder_write)\n",
    "    print(sorted(condensed_dict[keys_of_interest[0]].keys()))\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04b26716-0447-4415-a8b5-05fd6e24e26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#condense_exp(alphas, negative_fixes, norm)\n",
    "#generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0ef05-3cc8-4d79-8ff9-5ceb372f9c3b",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f927983b-3435-4d66-a2dd-b514b6e971e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input-cluster', 'validation-features-cluster', 'training-features-cluster', 'input-validation-features-cluster', 'input-training-features-cluster']\n",
      "NEGATIVE_VALUE\n",
      "[[2, 3, 4], [5, 6, 7], [8, 9, 10]]\n",
      "[[2, 3, 4], [5, 6, 7], [8, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "types = [\"input-cluster\", \"validation-features-cluster\", \"training-features-cluster\", \"input-validation-features-cluster\", \"input-training-features-cluster\"]\n",
    "print(types)\n",
    "alpha = [-1]\n",
    "\n",
    "tail_size = [6000]\n",
    "dist_multpl = [1.3]\n",
    "normy = norm[2]\n",
    "fix = negative_fixes[1]\n",
    "print(fix)\n",
    "\n",
    "input_cluster = np.arange(2, 11, 1).tolist()\n",
    "feature_cluster = np.arange(2, 11, 1).tolist()\n",
    "\n",
    "n = 3\n",
    "per_file_input = [input_cluster[i:i + n] for i in range(0, len(input_cluster), n)]\n",
    "per_file_feature = [feature_cluster[i:i + n] for i in range(0, len(feature_cluster), n)]\n",
    "print(per_file_input)\n",
    "print(per_file_feature)\n",
    "\n",
    "feature_clusters = np.arange(2, 10, 1)\n",
    "path = \"../configs/cluster/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85a9f2de-ddc5-4b9a-a7e1-3b0e5c18dd17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_cluster():\n",
    "    for t in types:\n",
    "        for i in per_file_input:\n",
    "            for f in per_file_feature:\n",
    "                options['type'] = t\n",
    "                options['alphas'] = alpha\n",
    "                options['tail_sizes'] = tail_size\n",
    "                options['distance_multipls'] = dist_multpl\n",
    "                options['negative_fix'] = fix\n",
    "                options['normalize_factor'] = normy\n",
    "                options['num_clusters_per_class_input'] = i if \"input\" in t else [1]\n",
    "                options['num_clusters_per_class_features'] = f if \"features\" in t else [1]\n",
    "                options['negative_fix'] = fix\n",
    "                if t == \"input-cluster\": options['eval_only'] = \"false\"\n",
    "                dict_to_file(options, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1375cc0-2e92-442d-99b4-e70ab2280124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openset-imagenet-comparison",
   "language": "python",
   "name": "openset-imagenet-comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
